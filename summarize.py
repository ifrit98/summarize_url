import os
import requests
from typing import Union


import openai # pip install openai
from langchain.text_splitter import CharacterTextSplitter, RecursiveCharacterTextSplitter

from langchain.chains.summarize import load_summarize_chain # pip install langchain
from langchain import OpenAI, PromptTemplate
from langchain.text_splitter import CharacterTextSplitter
from langchain.prompts import PromptTemplate

from modules.file import SimpleDirectoryReader


import argparse

parser = argparse.ArgumentParser(description='Download a PDF from a URL and generate a summary of its contents.')

parser.add_argument('-a', '--api_key', type=str, required=True, help="Your OpenAI API Key.")
parser.add_argument('-u', '--url', type=str, help='The URL of the PDF file to download.')
parser.add_argument('-f', '--file_name', type=str, help='The name to use for the downloaded PDF file.')
parser.add_argument('-o', '--summary_out', type=str, help='The path to save the generated summary.')
parser.add_argument('-m', '--max_tokens', type=int, default=512, help='The maximum number of tokens to use for the summary.')
parser.add_argument('-t', '--temperature', type=float, default=0, help='Temperature to use for LLM.')
parser.add_argument('-r', '--recursive_text_splitter', action='store_true', default=False, type=bool)

args = parser.parse_args()

os.environ['OPENAI_API_KEY'] = args.api_key
openai.api_key = args.api_key

MAX_CONTEXT_LENGTH = 4096
MAX_CHUNK_SIZE = (MAX_CONTEXT_LENGTH - args.max_tokens) - 128


prompt_template = """Write a detailed summary of the following in (at least) {} words:


{text}


CONCISE SUMMARY:""".format(args.max_tokens, text="{text}")
PROMPT = PromptTemplate(template=prompt_template, input_variables=["text"])

refine_template = (
    "Your job is to produce a final summary\n"
    "We have provided an existing summary up to a certain point: {existing_answer}\n"
    "We have the opportunity to refine the existing summary"
    "(only if needed) with some more context below.\n"
    "------------\n"
    "{text}\n"
    "------------\n"
    "Given the new context, refine the original summary and be sure to include new relevant details"
    "If the context isn't useful, return the original summary."
)
refine_prompt = PromptTemplate(
    input_variables=["existing_answer", "text"],
    template=refine_template,
)


def summarize(docs, chain_type="refine"):
    """
    Generates a summary of the given documents using a pre-trained language model.

    Args:
        docs (list): A list of document objects to summarize.

    Returns:
        str: A summary of the given documents generated by the language model.
    """
    assert chain_type in ["refine", "map_reduce"], "Must be one of `map_reduce`, `refine`"

    if chain_type == "refine":
        chain = load_summarize_chain(
            OpenAI(temperature=0, max_tokens=args.max_tokens), 
            chain_type="refine", 
            return_intermediate_steps=True, 
            question_prompt=PROMPT, 
            refine_prompt=refine_prompt
        )

    elif chain_type == "map_reduce":
        chain = load_summarize_chain(
            llm=OpenAI(temperature=0, max_tokens=args.max_tokens), 
            chain_type="map_reduce",
            map_prompt=PROMPT, 
            combine_prompt=PROMPT
        )

    return chain({"input_documents": docs}, return_only_outputs=True)


def download_pdf(url: str, file_name: str, tmpdir='pdf') -> Union[str, None]:
    """
    Downloads a PDF file from a given URL and saves it to the local file system with a given file name.

    Args:
        url (str): The URL of the PDF file to download.
        file_name (str): The name to use for the downloaded PDF file.

    Returns:
        bool: True if the PDF file was downloaded successfully, False otherwise.
    """

    outpath = os.path.join(tmpdir, file_name)
    if not os.path.exists(tmpdir): os.mkdir(tmpdir)

    try:
        response = requests.get(url)
        response.raise_for_status()

        with open(outpath, 'wb') as f:
            f.write(response.content)
            print(f'PDF downloaded successfully to {outpath}!')
        
        file_size = os.path.getsize(outpath)
        if file_size == 0:
            os.remove(outpath)
            print(f'Empty PDF file downloaded. Removed file {outpath}.')
        else:
            print(f'Downloaded PDF file size: {file_size} bytes')
            return outpath
    
    except requests.exceptions.HTTPError as e:
        print(f'HTTP error occurred: {e}')
    except requests.exceptions.ConnectionError as e:
        print(f'Connection error occurred: {e}')
    except requests.exceptions.Timeout as e:
        print(f'Timeout error occurred: {e}')
    except requests.exceptions.RequestException as e:
        print(f'An error occurred: {e}')
    
    return None


# Download the pdf using requests
path = download_pdf(args.url, file_name=args.file_name)

# Load PDF documents using gpt_index (from my personal repo)
loader = SimpleDirectoryReader(os.path.dirname(path), recursive=False, exclude_hidden=True)
documents = loader.load_data()

# Split docs - Choose your splitter
if args.recursive_text_splitter:
    text_splitter = RecursiveCharacterTextSplitter(
        chunk_size=MAX_CHUNK_SIZE, chunk_overlap=0
    )
else:
    text_splitter = CharacterTextSplitter(
        chunk_size=MAX_CHUNK_SIZE, chunk_overlap=0, separator="\n"
    )

# Convert to LangChain format from GPT Index. (artifact from SimpleDirectoryReader)
documents = [doc.to_langchain_format() for doc in documents]

# Split the docs into digestible chunks
docs = text_splitter.split_documents(documents)

# Do summarization and return
res = summarize(docs=docs, chain_type="refine")
out = res['output_text']
print(out)

# Write the summary to disk.
with open(args.summary_out, 'w') as f:
    f.writelines(out)
